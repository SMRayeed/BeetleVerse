{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9693b544-22dc-41b9-bd6e-581eb3c8e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d304ca9d-f60f-44c7-8b99-9101a1a06944",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"google/siglip-base-patch16-224\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name).to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25d6005-09a1-4e8f-98a6-825689ecf191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageFilePath</th>\n",
       "      <th>ScientificName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/fs/ess/PAS2136/Rayeed/Carabids-100K-V2/Insect...</td>\n",
       "      <td>Cicindela sexguttata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/fs/ess/PAS2136/Rayeed/Carabids-100K-V2/Insect...</td>\n",
       "      <td>Cicindela sexguttata</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       ImageFilePath        ScientificName\n",
       "0  /fs/ess/PAS2136/Rayeed/Carabids-100K-V2/Insect...  Cicindela sexguttata\n",
       "1  /fs/ess/PAS2136/Rayeed/Carabids-100K-V2/Insect...  Cicindela sexguttata"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/fs/ess/PAS2136/Rayeed/Carabids-100K-V2/AllCarabids.csv\"\n",
    "insect_df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "insect_df.rename(columns={\"filepath\": \"ImageFilePath\", \"genus\" : \"Genus\", \"class\": \"ScientificName\"}, inplace=True)\n",
    "insect_df['ImageFilePath'] = \"/fs/ess/PAS2136/Rayeed/Carabids-100K-V2/\" + insect_df['ImageFilePath']\n",
    "df = insect_df[['ImageFilePath', 'ScientificName']]\n",
    "df = df.fillna(\"Unknown\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b99cb76-2ba7-4dbe-8eb4-2ee3588d86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path) :\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "    \n",
    "    return features.cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2313e6-cf4e-48f1-b571-b2b100ed8091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ...:  55%|█████▍    | 52873/96530 [18:31<10:09, 71.64it/s]   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m df_unknown \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScientificName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract features for known samples (only passing img)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_known \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([extract_features(img) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(df_known[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageFilePath\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Encode labels only for known species\u001b[39;00m\n\u001b[1;32m      9\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m df_unknown \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScientificName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Extract features for known samples (only passing img)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m X_known \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(df_known[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImageFilePath\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Encode labels only for known species\u001b[39;00m\n\u001b[1;32m      9\u001b[0m le \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_features\u001b[39m(image_path) :\n\u001b[1;32m      3\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      8\u001b[0m         features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_image_features(pixel_values\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/myenv/lib64/python3.9/site-packages/transformers/models/siglip/processing_siglip.py:113\u001b[0m, in \u001b[0;36mSiglipProcessor.__call__\u001b[0;34m(self, text, images, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    108\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    109\u001b[0m         text, return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, padding\u001b[38;5;241m=\u001b[39mpadding, truncation\u001b[38;5;241m=\u001b[39mtruncation, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mpixel_values\n",
      "File \u001b[0;32m~/myenv/lib64/python3.9/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib64/python3.9/site-packages/transformers/utils/generic.py:854\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    851\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    852\u001b[0m     )\n\u001b[0;32m--> 854\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib64/python3.9/site-packages/transformers/models/siglip/image_processing_siglip.py:219\u001b[0m, in \u001b[0;36mSiglipImageProcessor.preprocess\u001b[0;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, do_convert_rgb)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    218\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 219\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    220\u001b[0m         resize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39m(height, width), resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    222\u001b[0m     ]\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    225\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    228\u001b[0m     ]\n",
      "File \u001b[0;32m~/myenv/lib64/python3.9/site-packages/transformers/models/siglip/image_processing_siglip.py:220\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m    218\u001b[0m     height, width \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m], size[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    219\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 220\u001b[0m         \u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    222\u001b[0m     ]\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_rescale:\n\u001b[1;32m    225\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    228\u001b[0m     ]\n",
      "File \u001b[0;32m~/myenv/lib64/python3.9/site-packages/transformers/image_transforms.py:341\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[1;32m    339\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    340\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m resized_image \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreducing_gap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreducing_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_numpy:\n\u001b[1;32m    344\u001b[0m     resized_image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(resized_image)\n",
      "File \u001b[0;32m~/myenv/lib64/python3.9/site-packages/PIL/Image.py:2356\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2344\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2345\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2346\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2347\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2348\u001b[0m         )\n\u001b[1;32m   2349\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2350\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2351\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2352\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2353\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2354\u001b[0m         )\n\u001b[0;32m-> 2356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Separate known and unknown samples\n",
    "df_known = df[df[\"ScientificName\"] != \"Unknown\"]\n",
    "df_unknown = df[df[\"ScientificName\"] == \"Unknown\"]\n",
    "\n",
    "# Extract features for known samples (only passing img)\n",
    "X_known = np.vstack([extract_features(img) for img in tqdm(df_known[\"ImageFilePath\"], desc=\"Processing ...\")])\n",
    "\n",
    "# Encode labels only for known species\n",
    "le = LabelEncoder()\n",
    "y_known = le.fit_transform(df_known[\"ScientificName\"])\n",
    "\n",
    "df_indices_known = df_known.index\n",
    "\n",
    "# Train-test split only on known species\n",
    "X_train, X_test_known, y_train, y_test_known, train_idx, test_idx_known = train_test_split(\n",
    "    X_known, y_known, df_indices_known, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Extract features for unknown samples (only passing img)\n",
    "X_test_unknown = np.vstack([extract_features(img) for img in tqdm(df_unknown[\"ImageFilePath\"], desc=\"Processing Unknown ...\")])\n",
    "test_idx_unknown = df_unknown.index\n",
    "\n",
    "# Assign a placeholder label (-1) for unknown samples\n",
    "y_test_unknown = np.full(len(X_test_unknown), -1)\n",
    "\n",
    "# Combine known and unknown test sets\n",
    "X_test = np.vstack([X_test_known, X_test_unknown])\n",
    "y_test = np.concatenate([y_test_known, y_test_unknown])\n",
    "test_idx = np.concatenate([test_idx_known, test_idx_unknown])\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "test_df = df.loc[test_idx].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0381a92-4faa-4c6a-9d27-f736a49e0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 99\n",
    "\n",
    "models = {\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=100),\n",
    "    \"NearestNeighbor\": KNeighborsClassifier(n_neighbors=5), \n",
    "    \"MLP_Baseline\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, random_state=seed)\n",
    "}\n",
    "\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    preds = model.predict(X_test_scaled)\n",
    "    predictions[name] = preds\n",
    "    \n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "    rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "    bal_acc = balanced_accuracy_score(y_test, preds)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    \n",
    "    metrics[name] = {\"Model\": name, \"Accuracy\": acc, \"Precision\": prec, \"Recall\": rec, \"F1-Score\": f1, \"Balanced Acc\": bal_acc, \"MCC\": mcc}\n",
    "    print(f\"{name:<25} | Acc: {acc:.2%} | Prec: {prec:.2%} | Rec: {rec:.2%} | F1: {f1:.2%} | Bal Acc: {bal_acc:.2%} | MCC: {mcc:.4f}\")\n",
    "\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436101d-307d-405f-8c0f-ca33b014add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.assign(**{f\"Pred_{name}\": le.inverse_transform(pred) for name, pred in predictions.items()})\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1015575-2991-44b6-b748-6c2993af6cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d624d-dd05-4e2c-92b1-ec08ab1c12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv(\"/users/PAS2136/rayees/CV4A - Benchmarking/AllCarabids/10.SigLIP-linear-probing-species.csv\", index=False)\n",
    "metrics_df.to_csv(\"/users/PAS2136/rayees/CV4A - Benchmarking/AllCarabids/10.SigLIP-linear-probing-species-metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2179e5a-001d-4da4-b603-4c57878f5e90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
